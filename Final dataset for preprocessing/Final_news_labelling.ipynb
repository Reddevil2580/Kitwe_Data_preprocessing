{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FAf9TEzAwCgw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from textblob import TextBlob\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "e8gqnfU3wDr2"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('kitwe_final_Clean.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nDBF8ZqmwSnZ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Source Credibility Analysis (updated with Zambian and Kitwe sources)\n",
        "reputable_sources = [\n",
        "    'bbc.com', 'reuters.com', 'nytimes.com', 'cnn.com', 'guardian.com', 'npr.org', 'forbes.com',\n",
        "    'bloomberg.com', 'washingtonpost.com', 'thetimes.co.uk', 'economist.com', 'wsj.com', 'cnbc.com'\n",
        "]\n",
        "\n",
        "# Including Zambian and Kitwe reputable sources\n",
        "zambian_reputable_sources = reputable_sources + [\n",
        "    'daily-mail.co.zm', 'times.co.zm', 'znbc.co.zm', 'flavaradioandtv.com', 'lusakatimes.com', 'kitwetimes.com'\n",
        "]\n",
        "suspicious_domain_pattern = re.compile(r'\\.(info|lo|ru|cn|xyz|top|news|live|buzz|click|online)$')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZnX2DC6IwXei"
      },
      "outputs": [],
      "source": [
        "def updated_check_source_credibility(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    domain = parsed_url.netloc.lower()\n",
        "    if any(source in domain for source in zambian_reputable_sources):\n",
        "        return 'reputable'\n",
        "    elif suspicious_domain_pattern.search(domain):\n",
        "        return 'suspicious'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "data['source_credibility'] = data['Link'].apply(updated_check_source_credibility)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "viKf5TEZwaIc"
      },
      "outputs": [],
      "source": [
        "# Step 2: Clickbait Analysis\n",
        "def detect_clickbait(headline):\n",
        "    excessive_punctuation = len(re.findall(r'[!?.]{2,}', headline)) > 0\n",
        "    all_caps = headline.isupper()\n",
        "    provocative_words = any(word in headline.lower() for word in ['shocking', 'unbelievable', 'you won’t believe', 'secret', 'amazing', 'incredible'])\n",
        "    if excessive_punctuation or all_caps or provocative_words:\n",
        "        return 'clickbait'\n",
        "    else:\n",
        "        return 'not_clickbait'\n",
        "\n",
        "data['headline_type'] = data['Headlines'].apply(detect_clickbait)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ocJh0S3owdtn"
      },
      "outputs": [],
      "source": [
        "# Step 3: Sensational Keyword Frequency Analysis\n",
        "sensational_keywords = ['shocking', 'unbelievable', 'amazing', 'incredible', 'secret', 'exposed', 'you won’t believe', 'scandal', 'controversy']\n",
        "def count_sensational_keywords(description):\n",
        "    count = sum(description.lower().count(word) for word in sensational_keywords)\n",
        "    return count\n",
        "\n",
        "data['sensational_keyword_count'] = data['Description'].apply(count_sensational_keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xsCTaHouwfzP"
      },
      "outputs": [],
      "source": [
        "# Step 4: Topic Modeling Using LDA\n",
        "count_vectorizer = CountVectorizer(max_features=300, stop_words='english')\n",
        "count_data = count_vectorizer.fit_transform(data['Description'].astype(str))\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda.fit(count_data)\n",
        "topic_distribution = lda.transform(count_data)\n",
        "data['dominant_topic'] = topic_distribution.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cq2oiEhSwhzc"
      },
      "outputs": [],
      "source": [
        "# Step 5: Sentiment Analysis\n",
        "def get_sentiment_score_textblob(text):\n",
        "    try:\n",
        "        sentiment = TextBlob(text).sentiment\n",
        "        return sentiment.polarity\n",
        "    except Exception as e:\n",
        "        return 0  # Default to neutral if there's an issue with the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "d-ITdMpWxeLa"
      },
      "outputs": [],
      "source": [
        "# 'sentiment_score' is calculated\n",
        "if 'sentiment_score' not in data.columns:\n",
        "    data['sentiment_score'] = data['Description'].apply(lambda x: get_sentiment_score_textblob(str(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_UiMCL2LxgSS"
      },
      "outputs": [],
      "source": [
        "def check_mismatch_headline_description(row):\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    combined_text = [row['Headlines'], row['Description']]\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(combined_text)\n",
        "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "    return similarity[0][0] < 0.3  # Low similarity indicates a mismatch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "kIlOEf21xi2o"
      },
      "outputs": [],
      "source": [
        "def check_excessive_capitalization(text):\n",
        "    words = text.split()\n",
        "    capitalized_words = [word for word in words if word.isupper() and len(word) > 1]\n",
        "    return len(capitalized_words) > 3  # Threshold of 3 or more fully capitalized words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "MNoC4SN-xlm7"
      },
      "outputs": [],
      "source": [
        "def check_vague_author(author):\n",
        "    vague_authors = ['admin', 'editor', 'newsroom', 'staff', 'unknown']\n",
        "    return any(vague_name in author.lower() for vague_name in vague_authors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "YuAicrCCxoXg"
      },
      "outputs": [],
      "source": [
        "def count_suspicious_links(description):\n",
        "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', description)\n",
        "    return len(urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "uDv4CnZyxrei"
      },
      "outputs": [],
      "source": [
        "def check_short_sensational_description(description):\n",
        "    description_length = len(description)\n",
        "    sensational_word_count = sum(description.lower().count(word) for word in sensational_keywords)\n",
        "    return description_length < 100 and sensational_word_count > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "NiwwpxFQxtyu"
      },
      "outputs": [],
      "source": [
        "# Apply new criteria to dataset\n",
        "data['excessive_capitalization'] = data['Headlines'].apply(check_excessive_capitalization)\n",
        "data['headline_description_mismatch'] = data.apply(check_mismatch_headline_description, axis=1)\n",
        "data['vague_author'] = data['Author'].apply(check_vague_author)\n",
        "data['suspicious_links_count'] = data['Description'].apply(count_suspicious_links)\n",
        "data['short_sensational_description'] = data['Description'].apply(check_short_sensational_description)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "YRpCMiTq2fKn"
      },
      "outputs": [],
      "source": [
        "# Step 7: Consolidation and Enhanced Logical Labeling\n",
        "def enhanced_determine_fake_news(row):\n",
        "    fake_indicators = 0\n",
        "\n",
        "    # Checking each feature, including the newly added ones\n",
        "    if row['source_credibility'] == -1:  # Suspicious source\n",
        "        fake_indicators += 1\n",
        "    if row['headline_type'] == 1:  # Clickbait headline\n",
        "        fake_indicators += 1\n",
        "    if row['sensational_keyword_count'] > 2:  # High count of sensational keywords\n",
        "        fake_indicators += 1\n",
        "    if row['dominant_topic'] in [1, 2]:  # If dominant topic appears to be sensational or dubious\n",
        "        fake_indicators += 1\n",
        "    if row['sentiment_score'] < -0.5 or row['sentiment_score'] > 0.5:  # Extreme sentiment\n",
        "        fake_indicators += 1\n",
        "    if row['excessive_capitalization']:  # Excessive capitalization\n",
        "        fake_indicators += 1\n",
        "    if row['headline_description_mismatch']:  # Headline-Description mismatch\n",
        "        fake_indicators += 1\n",
        "    if row['vague_author']:  # Vague author\n",
        "        fake_indicators += 1\n",
        "    if row['suspicious_links_count'] > 2:  # Multiple suspicious links\n",
        "        fake_indicators += 1\n",
        "    if row['short_sensational_description']:  # Short and sensational description\n",
        "        fake_indicators += 1\n",
        "\n",
        "    # Label as fake if 4 or more indicators are present\n",
        "    return 0 if fake_indicators >= 2 else 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "mkb1fiAc3YiK"
      },
      "outputs": [],
      "source": [
        "# Apply the enhanced function to determine the final label\n",
        "data['Target_final'] = data.apply(enhanced_determine_fake_news, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "6QtQOOc23ahK"
      },
      "outputs": [],
      "source": [
        "# Save the enhanced dataset with the new criteria applied\n",
        "data.to_csv('1.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ooty5pN93eHj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
